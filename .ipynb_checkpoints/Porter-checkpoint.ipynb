{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys, time\n",
    "from itertools import cycle\n",
    "\n",
    "def create_connection(db_file):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file, timeout=10)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return None\n",
    "\n",
    "\n",
    "        \n",
    "def postag(kata):\n",
    "    response = requests.get(\"https://kbbi.kemdikbud.go.id/entri/\"+kata)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    results = soup.find_all('div', attrs={'class': 'container body-content'})\n",
    "    result = results[0]\n",
    "    try:\n",
    "        if (result.find('ol')):\n",
    "            ol = result.find('ol')\n",
    "            return ol.li.i.span.text\n",
    "        else:\n",
    "            ul = result.find('ul')\n",
    "            return ul.li.i.span.text\n",
    "    except:\n",
    "        return 'Entri tidak ditemukan'\n",
    "    \n",
    "def firstRule(token):\n",
    "    \"fungsi stemming menghapus partikel -kah, -lah, -tah, -pun\"\n",
    "\n",
    "    # affix -kah\n",
    "    if re.search('([a-z0-9]+)kah$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)kah$', r'\\1', token)\n",
    "        return token\n",
    "\n",
    "    # affix -lah\n",
    "    if re.search('([a-z0-9]+)lah$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)lah$', r'\\1', token)\n",
    "        return token\n",
    "\n",
    "    # affix -tah\n",
    "    if re.search('([a-z0-9]+)tah$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)tah$', r'\\1', token)\n",
    "        return token\n",
    "\n",
    "    # affix -pun\n",
    "    if re.search('([a-z0-9]+)pun$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)pun$', r'\\1', token)\n",
    "        return token\n",
    "    return token\n",
    "\n",
    "def secondRule(token):\n",
    "    \"fungsi stemming menghapus kata ganti milik -ku, -mu, -nya\"\n",
    "\n",
    "    # affix -nya\n",
    "    if re.search('([a-z0-9]+)nya$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)nya$', r'\\1', token)\n",
    "        return token\n",
    "\n",
    "    # affix -ku\n",
    "    if re.search('([a-z0-9]+)ku$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)ku$', r'\\1', token)\n",
    "        return token\n",
    "\n",
    "    # affix -mu\n",
    "    if re.search('([a-z0-9]+)mu$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)mu$', r'\\1', token)\n",
    "        return token\n",
    "    return token\n",
    "\n",
    "\n",
    "def thirdRule(token):\n",
    "    \"fungsi stemming menghapus awalan me-, pe-, di-, ter-, ke-\"\n",
    "\n",
    "    # affix meng-\n",
    "    if re.search('^meng',token):\n",
    "        token = re.sub('^meng', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix meny-\n",
    "    if re.search('^meny',token):\n",
    "        token = \"s\"+token[4:len(token)]\n",
    "        #token = re.sub('^meny', 's', token)\n",
    "        return token\n",
    "\n",
    "    # affix men-\n",
    "    if re.search('^men',token):\n",
    "        token = re.sub('^men', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix me-\n",
    "    if re.search('^me',token):\n",
    "        token = re.sub('^me', '', token)\n",
    "        if re.search('^m([^aiueo])',token):\n",
    "            token = re.sub('^m([^aiueo])',r'\\1',token)\n",
    "        return token\n",
    "\n",
    "    # # affix mem-\n",
    "    if re.search('^mem',token):\n",
    "         if re.search('^mem[aiueo]',token):\n",
    "             token = re.sub('^mem', 'p', token)\n",
    "         else:\n",
    "             token = re.sub('^mem', '', token)\n",
    "         return token\n",
    "    \n",
    "    # # affix me-\n",
    "    # if re.search('^me',token):\n",
    "    #     token = re.sub('^me', '', token)\n",
    "    #     return token\n",
    "\n",
    "    # affix peng-\n",
    "    if re.search('^peng',token):\n",
    "        token = re.sub('^peng', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix peny-\n",
    "    if re.search('^peny',token):\n",
    "        token = re.sub('^peny', 's', token)\n",
    "        return token\n",
    "\n",
    "    # affix pen-\n",
    "    if re.search('^pen',token):\n",
    "        token = re.sub('^pen', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix pem-\n",
    "    if re.search('^pem',token):\n",
    "        if re.search('^pem[aiueo]',token):\n",
    "            token = re.sub('^pem', 'p', token)\n",
    "        else:\n",
    "            token = re.sub('^pem', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix di-\n",
    "    if re.search('^di',token):\n",
    "        token = re.sub('^di', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix ter-\n",
    "    if re.search('^ter',token):\n",
    "        token = re.sub('^ter', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix ke-\n",
    "    if re.search('^ke',token):\n",
    "        token = re.sub('^ke', '', token)\n",
    "        return token\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def fourthRule(token):\n",
    "    \"fungsi stemming menghapus awalan be-, pe-\"\n",
    "\n",
    "    # affix ber-\n",
    "    if re.search('^ber',token):\n",
    "        token = re.sub('^ber', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix belajar\n",
    "    if re.search('^belajar',token):\n",
    "        token = re.sub('^belajar', 'ajar', token)\n",
    "        return token\n",
    "\n",
    "    # affix bek+er-\n",
    "    if re.search('^bek+er',token):\n",
    "        token = re.sub('^bek+er', 'ker', token)\n",
    "        return token\n",
    "\n",
    "    # affix per-\n",
    "    if re.search('^per',token):\n",
    "        token = re.sub('^per', '', token)\n",
    "        return token\n",
    "\n",
    "    # affix pelajar\n",
    "    if re.search('^pelajar',token):\n",
    "        token = re.sub('^pelajar', 'ajar', token)\n",
    "        return token\n",
    "\n",
    "    # affix pe-\n",
    "    if re.search('^pe',token):\n",
    "        token = re.sub('^pe', '', token)\n",
    "        return token\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def fifthRule(token):\n",
    "    \"fungsi stemming menghapus akhiran -kan, -an, -i\"\n",
    "\n",
    "    # affix -kan\n",
    "    if re.search(r'([a-z0-9]+)kan$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)kan$', r'\\1', token)\n",
    "        return token\n",
    "\n",
    "    # affix -an\n",
    "    if re.search(r'([a-z0-9]+)an$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)an$', r'\\1', token)\n",
    "        return token\n",
    "\n",
    "    # affix -i\n",
    "    if re.search(r'([a-z0-9]+)i$',token):\n",
    "        token = re.sub(r'([a-z0-9]+)i$', r'\\1', token)\n",
    "        return token\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def stemming(token, rootword):\n",
    "    \"fungsi stemming menggunakan algoritma Porter untuk Bahasa Indonesia\"\n",
    "\n",
    "    # porter stemmer\n",
    "    # 1. Menghapus partikel seperti: -kah, -lah, -tah, -pun\n",
    "    # 2. Mengapus kata ganti (Possesive Pronoun), seperti -ku, -mu, -nya\n",
    "    # 3. Mengapus awalan pertama. Jika tidak ditemukan, maka lanjut ke langkah 4a, dan jika ada  maka lanjut ke langkah 4b.\n",
    "    # 4 .a. Menghapus awalan kedua, dan dilanjutkan pada langkah ke 5a\n",
    "    #    b. Menghapus akhiran, jika tidak ditemukan maka kata tersebut diasumsikan sebagai kata  dasar (root word). Jika ditemukan maka lanjut ke langkah 5b.\n",
    "    # 5. a. Menghapus akhiran dan kata akhir diasumsikan sebagai kata dasar (root word.\n",
    "    #    b. Menghapus awalan kedua dan kata akhir diasumsikan sebagai kata dasar (root word).\n",
    "\n",
    "\n",
    "    # 1. ---------------------------\n",
    "    token = firstRule(token)\n",
    "    if token in rootword:\n",
    "        return token\n",
    "\n",
    "    # 2. ---------------------------\n",
    "    token = secondRule(token)\n",
    "    if token in rootword:\n",
    "        return token\n",
    "\n",
    "    # 3. ---------------------------\n",
    "    tempToken = token\n",
    "    token = thirdRule(token)\n",
    "    if token in rootword:\n",
    "        return token\n",
    "    if tempToken==token:\n",
    "        token = fourthRule(token)\n",
    "        if token in rootword:\n",
    "            return token\n",
    "        token = fifthRule(token)\n",
    "        if token in rootword:\n",
    "            return token\n",
    "    else:\n",
    "        temptoken2 = token\n",
    "        token = fifthRule(token)\n",
    "        if token in rootword :\n",
    "            return token\n",
    "        if token != temptoken2:\n",
    "            token = fourthRule(token)\n",
    "            if token in rootword :\n",
    "                return token\n",
    "    # 4. ---------------------------\n",
    "    # 5. ---------------------------\n",
    "    return token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanyi\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "import openpyxl\n",
    "\n",
    "def insertDB(conn):\n",
    "    fileTrain = xlrd.open_workbook(\"data_clean.xlsx\")\n",
    "    dataTrain = fileTrain.sheet_by_index(0)\n",
    "    rowLen = dataTrain.nrows\n",
    "    \n",
    "    filePreprocessed = openpyxl.Workbook()\n",
    "    dataPreprocessed = filePreprocessed.active\n",
    "    \n",
    "    for i in range(rowLen):\n",
    "        rootword = [line.rstrip('\\n') for line in open('dictionary/rootword.txt')]\n",
    "        data_i = dataTrain.cell(i,0).value\n",
    "        class_i = dataTrain.cell(i, 1).value\n",
    "        word = stemming(data_i,rootword)\n",
    "        #tag = postag(word)\n",
    "        tag = \"n\"\n",
    "        if word:\n",
    "            if tag != \"Entri tidak ditemukan\":\n",
    "                dataPreprocessed.append([(word), tag])\n",
    "                cur = conn.cursor()\n",
    "                #c = cur.execute(\"select exists(select 1 from kataDasar where kata=?)\",(word,)).fetchone()[0]\n",
    "                cur.execute('INSERT OR IGNORE INTO dasar(kata,tag) VALUES (?,?) ', (word,tag))\n",
    "                conn.commit()\n",
    "                \n",
    "    filePreprocessed.save(\"kata_dasar.xlsx\")\n",
    "    #tag = postag(word)\n",
    "    #i=1\n",
    "    #cur = conn.cursor()\n",
    "    #cur.execute('INSERT INTO kataDasar(kata,tag) VALUES (?,?) ', (word,tag))\n",
    "    #conn.commit()\n",
    "    #conn.close()\n",
    "\n",
    "def main():\n",
    "#     database = \"E:\\DataBaseTA\\corpus.db\"\n",
    "#     conn = create_connection(database)\n",
    "#     with conn:\n",
    "#         insertDB(conn)\n",
    "    rootword = [line.rstrip('\\n') for line in open('dictionary/rootword.txt')]\n",
    "    #print(stemming(\"menyanyi\",rootword))\n",
    "    token = \"menyanyi\"\n",
    "    print(\"s\"+token[4:len(token)])\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
