{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import xlrd\n",
    "import openpyxl\n",
    "\n",
    "__author__ = 'undeed'\n",
    "\n",
    "\n",
    "def preprocessFile (filenameInput, filenamePreprocessed):\n",
    "    \"fungsi memproses file excel filenameInput, \" \\\n",
    "    \"dan menyimpan hasil preprocessing ke dalam file excel filenamePreprocessed\"\n",
    "\n",
    "    # buka file input\n",
    "    fileTrain = xlrd.open_workbook(filenameInput)\n",
    "    dataTrain = fileTrain.sheet_by_index(0)\n",
    "    rowLen = dataTrain.nrows\n",
    "    \n",
    "    # siapkan file output\n",
    "    filePreprocessed = openpyxl.Workbook()\n",
    "    dataPreprocessed = filePreprocessed.active\n",
    "\n",
    "    # untuk setiap data input, lakukan preprocessing\n",
    "    # dan hasil preprocessing simpan ke dalam data output\n",
    "    for i in range(rowLen):\n",
    "        data_i = dataTrain.cell(i,0).value\n",
    "        class_i = dataTrain.cell(i, 1).value\n",
    "        prep = preprocess(data_i)\n",
    "\n",
    "        if prep:\n",
    "            for i in range(len(prep)):\n",
    "                dataPreprocessed.append([''.join(prep[i]), class_i])\n",
    "\n",
    "    # simpan file output\n",
    "    filePreprocessed.save(filenamePreprocessed)\n",
    "    return dataPreprocessed\n",
    "\n",
    "\n",
    "def preprocess (article):\n",
    "    \"fungsi melakukan preprocessing sebuah data artikel \" \\\n",
    "    \"(normalisasi, stemming, tokenisasi) \" \\\n",
    "    \"dan mengembalikan list token hasil preprocessing\"\n",
    "\n",
    "    # case folding\n",
    "    article = article.lower()\n",
    "\n",
    "    # url removal\n",
    "    article = removeUrl(article)\n",
    "\n",
    "    # non alphabetic removal\n",
    "    article = re.sub(r'([a-z])-([a-z])', r'\\1 \\2', article)\n",
    "    article = re.sub(r'[^a-z|^ ]', '', article)\n",
    "\n",
    "    # tokenization\n",
    "    #tokens = article.split()\n",
    "\n",
    "    # coming soon - stemming\n",
    "    #rootword = [line.rstrip('\\n') for line in open('dictionary/rootword.txt')]\n",
    "\n",
    "    #nonKataDasar = list(set(tokens) - set(rootword))\n",
    "    #kataDasar = list(set(tokens) - set(nonKataDasar))\n",
    "\n",
    "    #for i in range(0, len(nonKataDasar)):\n",
    "        #nonKataDasar[i] = stemming(nonKataDasar[i],rootword)\n",
    "\n",
    "#    tokens = list(set(nonKataDasar + kataDasar))\n",
    "\n",
    "    # stopword removal\n",
    "    #result = stopwordRemoval(tokens)\n",
    "    return article\n",
    "\n",
    "\n",
    "def removeUrl(article):\n",
    "    \"fungsi menghapus URL di dalam sebuah artikel\"\n",
    "\n",
    "    def regex_or(*items):\n",
    "        r = '|'.join(items)\n",
    "        r = '(' + r + ')'\n",
    "        return r\n",
    "\n",
    "    def pos_lookahead(r):\n",
    "        return '(?=' + r + ')'\n",
    "\n",
    "    def neg_lookahead(r):\n",
    "        return '(?!' + r + ')'\n",
    "\n",
    "    def optional(r):\n",
    "        return '(%s)?' % r\n",
    "\n",
    "    PunctChars = r'''['\".?!,:;]'''\n",
    "    Punct = '%s+' % PunctChars\n",
    "    Entity = '&(amp|lt|gt|quot);'\n",
    "\n",
    "    UrlStart1 = regex_or(r'https?://?', r'www\\.')\n",
    "    CommonTLDs = regex_or('com', 'co\\\\.uk', 'org', 'net', 'info', 'ca')\n",
    "    UrlStart2 = r'[a-z0-9\\.-]+?' + r'\\.' + CommonTLDs + pos_lookahead(r'[/ \\W\\b]')\n",
    "    UrlBody = r'[^ \\t\\r\\n<>]*?'\n",
    "    UrlExtraCrapBeforeEnd = '%s+?' % regex_or(PunctChars, Entity)\n",
    "    UrlEnd = regex_or(r'\\.\\.+', r'[<>]', r'\\s', '$')\n",
    "    Url = (r'\\b' +\n",
    "           regex_or(UrlStart1, UrlStart2) +\n",
    "           UrlBody +\n",
    "           pos_lookahead(optional(UrlExtraCrapBeforeEnd) + UrlEnd))\n",
    "\n",
    "    Url_RE = re.compile(\"(%s)\" % Url, re.U | re.I)\n",
    "    article = re.sub(Url_RE, \"\", article)\n",
    "\n",
    "    return article\n",
    "\n",
    "\n",
    "def stopwordRemoval(tokens):\n",
    "    \"proses menghapus token-token yang tidak bermakna \" \\\n",
    "    \"berdasarkan daftar stopword\"\n",
    "\n",
    "    stopword = [line.rstrip('\\n') for line in open('dictionary/stopword.txt')]\n",
    "    stopword2 = [line.rstrip('\\n') for line in open('dictionary/dict_noise.txt')]\n",
    "    tokens2 = []\n",
    "    for ii in range(0, len(tokens)):\n",
    "        if tokens[ii] not in stopword and tokens[ii] not in stopword2:\n",
    "            tokens2.append(tokens[ii])\n",
    "    return tokens2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Worksheet \"Sheet\">"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDataTrain = 'data.xlsx'\n",
    "preprocessedData = \"data_clean.xlsx\"\n",
    "\n",
    "preprocessFile(inputDataTrain, preprocessedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
